@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NeurIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NeurIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{lei-etal-2023-revealing,
    title = "Revealing Single Frame Bias for Video-and-Language Learning",
    author = "Lei, Jie  and
      Berg, Tamara  and
      Bansal, Mohit",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "ACL",
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.29",
    doi = "10.18653/v1/2023.acl-long.29",
    abstract = "Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong {``}static appearance bias{''} in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at \url{https://github.com/jayleicn/singularity}.",
}

@article{xu2024mcbenchbenchmarkmulticontextvisual,
  title={MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs},
  author={Xu, Yunqiu and Zhu, Linchao and Yang, Yi},
  journal={arXiv preprint arXiv:2410.12332},
  year={2024}
}

@inproceedings{dai2017scannet,
  title={Scannet: Richly-annotated 3d reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{yeshwanth2023scannet++,
  title={Scannet++: A high-fidelity dataset of 3d indoor scenes},
  author={Yeshwanth, Chandan and Liu, Yueh-Cheng and Nie{\ss}ner, Matthias and Dai, Angela},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{
dehghan2021arkitscenes,
title={{ARK}itScenes - A Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile {RGB}-D Data},
author={Gilad Baruch and Zhuoyuan Chen and Afshin Dehghan and Tal Dimry and Yuri Feigin and Peter Fu and Thomas Gebauer and Brandon Joffe and Daniel Kurz and Arik Schwartz and Elad Shulman},
booktitle={NeurIPS},
year={2021},
url={https://openreview.net/forum?id=tjZjv_qh_CE}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{huang2023can,
  title={Can large language models explain themselves? a study of llm-generated self-explanations},
  author={Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2310.11207},
  year={2023}
}

@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={IJCV},
  year={2010}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014}
}

@book{salton1986introduction,
author = {Salton, Gerard and McGill, Michael J.},
title = {Introduction to Modern Information Retrieval},
year = {1986},
isbn = {0070544840},
publisher = {McGraw-Hill, Inc.},
address = {USA}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={NeurIPS},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={NeurIPS},
  year={2022}
}

@book{gardner1983frames,
  author       = {Howard Gardner},
  title        = {Frames of Mind: The Theory of Multiple Intelligences},
  year         = {1983},
  edition      = {Tenth-anniversary Edition, Second Paperback Edition},
  publisher    = {Basic Books}
}

@article{naveed2024comprehensiveoverviewlargelanguage,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}


@article{beguvs2023large,
  title={Large linguistic models: Analyzing theoretical linguistic abilities of LLMs},
  author={Begu{\v{s}}, Ga{\v{s}}per and D{\k{a}}bkowski, Maksymilian and Rhodes, Ryan},
  journal={arXiv preprint arXiv:2305.00948},
  year={2023}
}

@inproceedings{zhang-etal-2024-unveiling-linguistic,
    title = "Unveiling Linguistic Regions in Large Language Models",
    author = "Zhang, Zhihao  and
      Zhao, Jun  and
      Zhang, Qi  and
      Gui, Tao  and
      Huang, Xuanjing",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "ACL",
    year = "2024"
}


@article{52065,title	= {Emergent abilities of large language models},author	= {Barret Zoph and Colin Raffel and Dale Schuurmans and Dani Yogatama and Denny Zhou and Don Metzler and Ed H. Chi and Jason Wei and Jeff Dean and Liam B. Fedus and Maarten Paul Bosma and Oriol Vinyals and Percy Liang and Sebastian Borgeaud and Tatsunori B. Hashimoto and Yi Tay},year	= {2022},journal	= {TMLR}}

@inproceedings{Kassner2023LanguageMW,
  title={Language Models with Rationality},
  author={Nora Kassner and Oyvind Tafjord and Ashish Sabharwal and Kyle Richardson and Hinrich Sch{\"u}tze and Peter Clark},
  booktitle={EMNLP},
  year={2023}
}
@article{clark1991dual,
  author    = {Clark, James M. and Paivio, Allan},
  title     = {Dual Coding Theory and Education},
  journal   = {Educational Psychology Review},
  volume    = {3},
  number    = {3},
  pages     = {149--210},
  year      = {1991},
  doi       = {10.1007/BF01320076}
}

@incollection{meneghetti2022individual,
  author    = {Chiara Meneghetti and Laura Miola and Tommaso Feraco and Veronica Muffato},
  title     = {Individual Differences in Navigation: An Introductory Overview},
  booktitle = {Prime Archives in Psychology: 2nd Edition},
  editor    = {Paul Raj},
  year      = {2022},
  publisher = {Vide Leaf},
  address   = {Hyderabad, India}
}


@inbook{Newcombe2024Spatial,
	author = {Newcombe, Nora S.},
	booktitle = {Open {Encyclopedia} of {Cognitive} {Science}},
	editor = {Frank, Michael C. and Majid, Asifa},
	year = {2024},
	month = {jul 24},
	note = {https://oecs.mit.edu/pub/or750iar},
	publisher = {MIT Press},
	title = {Spatial {Cognition}},
}

@article{chabris_jerde_woolley_gerbasi_schuldt_wai_bennett_hackman_kosslyn_2023,
  title={Spatial and Object Visualization Cognitive Styles: Validation Studies in 3839 Individuals},
  author={Chabris, Christopher F and Jerde, Thomas and Woolley, Anita Williams and Gerbasi, Margaret E and Schuldt, Jonathon P and Wai, Jonathan and Bennett, Sean L and Hackman, J Richard and Kosslyn, Stephen M},
  year={2023},
  publisher={PsyArXiv}
}

@article{tolman1948cognitive,
  author    = {Tolman, E. C.},
  title     = {Cognitive maps in rats and men},
  journal   = {Psychological Review},
  year      = {1948},
  volume    = {55},
  number    = {4},
  pages     = {189--208},
  doi       = {10.1037/h0061626}
}

@article{mcafoose2009exploring,
  author    = {McAfoose, Julia and Baune, Bernhard T.},
  title     = {Exploring Visual–Spatial Working Memory: A Critical Review of Concepts and Models},
  journal   = {Neuropsychology Review},
  year      = {2009},
}

@book{dehn2011working,
  author    = {Dehn, Milton J.},
  title     = {Working Memory and Academic Learning: Assessment and Intervention},
  year      = {2011},
  publisher = {John Wiley \& Sons},
  pages     = {80},
  isbn      = {9781118045169}
}

@article{tong2024cambrian,
  title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng, and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@article{hurst2024gpto,
  title={GPT-4o System Card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@inproceedings{lin2024vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song},
  booktitle={CVPR},
  year={2024}
}

@article{xue2024longvila,
  title={Longvila: Scaling long-context visual language models for long videos},
  author={Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others},
  journal={arXiv preprint arXiv:2408.10188},
  year={2024}
}

@article{li2024llavaov,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@misc{zhang2024llavanextvideo,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  year={2024}
}

@inproceedings{
    jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={ICLR},
    year={2024},
    url={https://openreview.net/forum?id=VTF8yNQM66}
}

@inproceedings{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{52081,title	= {Self-Consistency Improves Chain of Thought Reasoning in Language Models},author	= {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V. Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou}, URL	= {https://arxiv.org/abs/2203.11171},booktitle	= {ICLR 2023}}

@inproceedings{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  booktitle={NeurIPS},
  year={2024}
}

@article{schulman2022chatgpt,
  title={{ChatGPT: Optimizing language models for dialogue}},
  author={Schulman, John and Zoph, Barret and Kim, Christina and Hilton, Jacob and Menick, Jacob and Weng, Jiayi and Uribe, Juan Felipe Ceron and Fedus, Liam and Metz, Luke and Pokorny, Michael and others},
  journal={OpenAI blog},
  year={2022}
}

@inproceedings{tian2024drivevlm,
  title={Drivevlm: The convergence of autonomous driving and large vision-language models},
  author={Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Wang, Yang and Zhao, Zhiyong and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang},
  booktitle={CoRL},
  year={2024}
}

@inproceedings{shao2024lmdrive,
  title={Lmdrive: Closed-loop end-to-end driving with large language models},
  author={Shao, Hao and Hu, Yuxuan and Wang, Letian and Song, Guanglu and Waslander, Steven L and Liu, Yu and Li, Hongsheng},
  booktitle={CVPR},
  pages={15120--15130},
  year={2024}
}

@inproceedings{brohan2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  booktitle={CoRL},
  year={2023}
}

@inproceedings{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  booktitle={RSS},
  year={2023}
}

@article{o2023open,
  title={Open x-embodiment: Robotic learning datasets and rt-x models},
  author={O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others},
  journal={arXiv preprint arXiv:2310.08864},
  year={2023}
}

@article{wang2023voyager,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={TMLR},
  year={2023}
}

@book{nadel2008hippocampus,
  author    = {Nadel, Lynn},
  title     = {The Hippocampus and Context Revisited},
  year      = {2008},
  publisher = {Oxford University Press},
  doi       = {10.1093/acprof:oso/9780195323245.001.0001},
  isbn      = {978-0-19-986926-8}
}

@inproceedings{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={ICML},
  year={2022}
}

@inproceedings{zhang-etal-2023-video,
    title = "Video-{LL}a{MA}: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
    author = "Zhang, Hang  and
      Li, Xin  and
      Bing, Lidong",
    editor = "Feng, Yansong  and
      Lefever, Els",
    booktitle = "EMNLP",
    year = "2023",
    url = "https://aclanthology.org/2023.emnlp-demo.49",
    doi = "10.18653/v1/2023.emnlp-demo.49",
    abstract = "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual {\&} audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual {\&} audio encoders with LLM{'}s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
}

@article{2023videochat,
  title={VideoChat: Chat-Centric Video Understanding},
  author={Li, Kunchang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@InProceedings{Ren_2024_CVPR,
    author    = {Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
    title     = {TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding},
    booktitle = {CVPR}
}

@inproceedings{huang2024vtimellm,
  title={Vtimellm: Empower llm to grasp video moments},
  author={Huang, Bin and Wang, Xin and Chen, Hong and Song, Zihan and Zhu, Wenwu},
  booktitle={CVPR},
  year={2024}
}
@INPROCEEDINGS{7298698,
  author={Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
  booktitle={CVPR}, 
  title={ActivityNet: A large-scale video benchmark for human activity understanding}, 
  year={2015},
  keywords={Benchmark testing;Taxonomy;Cleaning;Semantics;Organizations;Complexity theory;YouTube},
  doi={10.1109/CVPR.2015.7298698}
}


@article{Goyal2017TheS,
  title={The “Something Something” Video Database for Learning and Evaluating Visual Common Sense},
  author={Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and Susanne Westphal and Heuna Kim and Valentin Haenel and Ingo Fr{\"u}nd and Peter N. Yianilos and Moritz Mueller-Freitag and Florian Hoppe and Christian Thurau and Ingo Bax and Roland Memisevic},
  journal={ICCV},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:834612}
}

@InProceedings{Chen_2024_CVPR,
    author    = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
    title     = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
    booktitle = {CVPR},
    year      = {2024}
}
@inproceedings{cheng2024spatialrgpt,
      title={SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models},
      author={Cheng, An-Chieh and Yin, Hongxu and Fu, Yang and Guo, Qiushan and Yang, Ruihan and Kautz, Jan and Wang, Xiaolong and Liu, Sifei},
      booktitle={NeurIPS},
      year={2024}
}
  

@article{cai2024spatialbot,
  title={SpatialBot: Precise Spatial Understanding with Vision Language Models},
  author={Cai, Wenxiao and Ponomarenko, Yaroslav and Yuan, Jianhao and Li, Xiaoqi and Yang, Wankou and Dong, Hao and Zhao, Bo},
  journal={arXiv preprint arXiv:2406.13642},
  year={2024}
}

@article{liu2024coarsecorrespondenceelicit3d,
  title={Coarse correspondence elicit 3d spacetime understanding in multimodal language model},
  author={Liu, Benlin and Dong, Yuhao and Wang, Yiqin and Rao, Yongming and Tang, Yansong and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2408.00754},
  year={2024}
}

@article{yang2023setofmarkpromptingunleashesextraordinary,
      title={Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V}, 
      author={Jianwei Yang and Hao Zhang and Feng Li and Xueyan Zou and Chunyuan Li and Jianfeng Gao},
      journal={arXiv preprint arXiv:2310.11441},
      year={2023},
}


@article{fu2024mmecomprehensiveevaluationbenchmark,
  title={Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}


@inproceedings{liu2024mmbenchmultimodalmodelallaround,
  title={{MMBench: Is your multi-modal model an all-around player?}},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={ECCV},
  year={2024},
}


@inproceedings{yue2023mmmu,
    title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
    author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
    booktitle={CVPR},
    year={2024},
}

@inproceedings{li-etal-2024-multimodal-arxiv,
  title = "Multimodal {A}r{X}iv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models",
  author = "Li, Lei  and
    Wang, Yuqi  and
    Xu, Runxin  and
    Wang, Peiyi  and
    Feng, Xiachong  and
    Kong, Lingpeng  and
    Liu, Qi",
  editor = "Ku, Lun-Wei  and
    Martins, Andre  and
    Srikumar, Vivek",
  booktitle = "ACL",
  year = "2024",
  url = "https://aclanthology.org/2024.acl-long.775",
  doi = "10.18653/v1/2024.acl-long.775"
}

@article{fu2024video,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@InProceedings{Li_2024_CVPR,
    author    = {Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and Wang, Limin and Qiao, Yu},
    title     = {MVBench: A Comprehensive Multi-modal Video Understanding Benchmark},
    booktitle = {CVPR},
    month     = {June},
    year      = {2024},
    pages     = {22195-22206}
}

@article{ye2024mmegobuildingegocentricmultimodal,
  title={MM-Ego: Towards Building Egocentric Multimodal LLMs},
  author={Ye, Hanrong and Zhang, Haotian and Daxberger, Erik and Chen, Lin and Lin, Zongyu and Li, Yanghao and Zhang, Bowen and You, Haoxuan and Xu, Dan and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2410.07177},
  year={2024}
}


@inproceedings{liu-etal-2024-tempcompass,
    title = "{T}emp{C}ompass: Do Video {LLM}s Really Understand Videos?",
    author = "Liu, Yuanxin  and
      Li, Shicheng  and
      Liu, Yi  and
      Wang, Yuxiang  and
      Ren, Shuhuai  and
      Li, Lei  and
      Chen, Sishuo  and
      Sun, Xu  and
      Hou, Lu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of ACL",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-acl.517",
    doi = "10.18653/v1/2024.findings-acl.517",
    abstract = "Recently, there is a surge in interest surrounding video large language models (Video LLMs). However, existing benchmarks fail to provide a comprehensive feedback on the temporal perception ability of Video LLMs. On the one hand, most of them are unable to distinguish between different temporal aspects (e.g., speed, direction) and thus cannot reflect the nuanced performance on these specific aspects. On the other hand, they are limited in the diversity of task formats (e.g., only multi-choice QA), which hinders the understanding of how temporal perception performance may vary across different types of tasks. Motivated by these two problems, we propose the \textbf{TempCompass} benchmark, which introduces a diversity of temporal aspects and task formats. To collect high-quality test data, we devise two novel strategies: (1) In video collection, we construct conflicting videos that share the same static content but differ in a specific temporal aspect, which prevents Video LLMs from leveraging single-frame bias or language priors. (2) To collect the task instructions, we propose a paradigm where humans first annotate meta-information for a video and then an LLM generates the instruction. We also design an LLM-based approach to automatically and accurately evaluate the responses from Video LLMs. Based on TempCompass, we comprehensively evaluate 9 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs, and reveal the discerning fact that these models exhibit notably poor temporal perception ability.",
}

@inproceedings{li2024vitatecsdiagnosticdatasettemporal,
  title={Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models},
  author={Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{chen2024rextimebenchmarksuitereasoningacrosstime,
  title={ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos},
  author={Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Yu-Chiang Frank},
  booktitle={NeurIPS},
  year={2024}
}

@article{shangguan2024tomatoassessingvisualtemporal,
  title={{TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models}},
  author={Shangguan, Ziyao and Li, Chuhan and Ding, Yuxuan and Zheng, Yanan and Zhao, Yilun and Fitzgerald, Tesca and Cohan, Arman},
  journal={arXiv preprint arXiv:2410.23266},
  year={2024}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={KDD},
  year={2016}
}

@article{xu2024drivegpt4,
  title={Drivegpt4: Interpretable end-to-end autonomous driving via large language model},
  author={Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K and Li, Zhenguo and Zhao, Hengshuang},
  journal={RA-L},
  year={2024},
  publisher={IEEE}
}

@inproceedings{chandrasegaran2024hourvideo,
      title={HourVideo: 1-Hour Video-Language Understanding},
      author={Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M. and Kota, Taran and 
      He, Jimming and Eyzaguirre, Cristobal and Durante, Zane and Li, Manling and Wu, Jiajun and Li, Fei-Fei},
      booktitle = {NeurIPS},
      year={2024},
      volume = {37},
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{gur2023real,
  title={A real-world webagent with planning, long context understanding, and program synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  booktitle={ICLR},
  year={2024}
}

@article{shepard1988mental,
  title={Mental rotation: effects of dimensionality of objects and type of task.},
  author={Shepard, Shenna and Metzler, Douglas},
  journal={Journal of experimental psychology: Human perception and performance},
  volume={14},
  number={1},
  pages={3},
  year={1988},
  publisher={American Psychological Association}
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle={NeurIPS},
  year={2020}
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{bai2023qwenvl,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={CVPR},
  year={2024}
}

@article{wang2024qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2022}
}


@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={ICCV},
  year={2023}
}

@article{mu2024embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={NeurIPS},
  year={2024}
}

@article{liu2024world,
  title={World model on million-length video and language with ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2402.08268},
  year={2024}
}

@inproceedings{kim2024openvla,
  title={OpenVLA: An Open-Source Vision-Language-Action Model},
  author={Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others},
  booktitle={CoRL},
  year={2024}
}

@inproceedings{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  year={2023}
}

@inproceedings{chen2024spatialvlm,
  title={Spatialvlm: Endowing vision-language models with spatial reasoning capabilities},
  author={Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  booktitle={ICML},
  year={2023}
}

@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{liu2025mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={ECCV},
  year={2025}
}

@inproceedings{li2024seed,
  title={SEED-Bench: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={CVPR},
  year={2024}
}

@article{yu2023mmvet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={ICML},
  year={2024}
}


@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
  year={2024}
}

@article{ning2023videobench,
  title={Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models},
  author={Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li},
  journal={arXiv preprint arXiv:2311.16103},
  year={2023}
}


@article{li2023vitatecs,
  title={Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models},
  author={Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2311.17404},
  year={2023}
}

@inproceedings{fang2024mmbenchvideo,
  title={MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
  booktitle={NeurIPS},
  year={2024}
}

@article{ye2024mmego,
  title={MM-Ego: Towards Building Egocentric Multimodal LLMs},
  author={Ye, Hanrong and Zhang, Haotian and Daxberger, Erik and Chen, Lin and Lin, Zongyu and Li, Yanghao and Zhang, Bowen and You, Haoxuan and Xu, Dan and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2410.07177},
  year={2024}
}

@article{li2024topviewrs,
  title={TopViewRS: Vision-Language Models as Top-View Spatial Reasoners},
  author={Li, Chengzu and Zhang, Caiqi and Zhou, Han and Collier, Nigel and Korhonen, Anna and Vuli{\'c}, Ivan},
  journal={arXiv preprint arXiv:2406.02537},
  year={2024}
}

@article{tang2024sparkle,
  title={Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning},
  author={Tang, Yihong and Qu, Ao and Wang, Zhaokai and Zhuang, Dingyi and Wu, Zhaofeng and Ma, Wei and Wang, Shenhao and Zheng, Yunhan and Zhao, Zhan and Zhao, Jinhua},
  journal={arXiv preprint arXiv:2410.16162},
  year={2024}
}

@inproceedings{yang2024virl,
  title={V-irl: Grounding virtual intelligence in real life},
  author={Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining},
  booktitle={ECCV},
  year={2024}
}

@article{ramakrishnan2024does,
  title={Does Spatial Cognition Emerge in Frontier Models?},
  author={Ramakrishnan, Santhosh Kumar and Wijmans, Erik and Kraehenbuehl, Philipp and Koltun, Vladlen},
  journal={arXiv preprint arXiv:2410.06468},
  year={2024}
}


@article{zhu2024llava3d,
  title={LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness},
  author={Zhu, Chenming and Wang, Tai and Zhang, Wenwei and Pang, Jiangmiao and Liu, Xihui},
  journal={arXiv preprint arXiv:2409.18125},
  year={2024}
}

@article{wu2024visualization,
  title={Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models},
  author={Wu, Wenshan and Mao, Shaoguang and Zhang, Yadong and Xia, Yan and Dong, Li and Cui, Lei and Wei, Furu},
  journal={NeurIPS},
  year={2024}
}

@article{buhner2008working,
  title={Working memory, visual--spatial-intelligence and their relationship to problem-solving},
  author={B{\"u}hner, Markus and Kr{\"o}ner, Stephan and Ziegler, Matthias},
  journal={Intelligence},
  volume={36},
  number={6},
  pages={672--680},
  year={2008},
  publisher={Elsevier}
}

@article{marshall2001spatial,
  title={Spatial cognition: where we were and where we are},
  author={Marshall, John C and Fink, Gereon R},
  journal={Neuroimage},
  volume={14},
  number={1},
  pages={S2--S7},
  year={2001},
  publisher={Elsevier}
}

@book{waller2013handbook,
  title={Handbook of spatial cognition.},
  author={Waller, David Ed and Nadel, Lynn Ed},
  year={2013},
  publisher={American Psychological Association}
}

@book{mallot2024geometry,
  title={From geometry to behavior: An introduction to spatial cognition},
  author={Mallot, Hanspeter A},
  year={2024},
  publisher={MIT Press}
}

@article{vasilyeva2012development,
  title={Development of spatial cognition},
  author={Vasilyeva, Marina and Lourenco, Stella F},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={3},
  number={3},
  pages={349--362},
  year={2012},
  publisher={Wiley Online Library}
}
@book{newcombe2000making,
  title={Making space: The development of spatial representation and reasoning},
  author={Newcombe, NS},
  year={2000},
  publisher={MIT Press}
}

@article{ren2016faster,
  title={Faster R-CNN: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={PAMI},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2016},
  publisher={IEEE}
}

@article{
yamada2024evaluating,
title={Evaluating Spatial Understanding of Large Language Models},
author={Yutaro Yamada and Yihan Bao and Andrew Kyle Lampinen and Jungo Kasai and Ilker Yildirim},
journal={TMLR},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=xkiflfKCw3},
note={}
}

@article{momennejad2024evaluating,
  title={Evaluating cognitive maps and planning in large language models with CogEval},
  author={Momennejad, Ida and Hasanbeig, Hosein and Vieira Frujeri, Felipe and Sharma, Hiteshi and Jojic, Nebojsa and Palangi, Hamid and Ness, Robert and Larson, Jonathan},
  journal={NeurIPS},
  year={2024}
}

@article{shepard1978mental,
  title={The mental image.},
  author={Shepard, Roger N},
  journal={American psychologist},
  volume={33},
  number={2},
  pages={125},
  year={1978},
  publisher={American Psychological Association}
}

@article{rajabi2023towards,
  title={Towards grounded visual spatial reasoning in multi-modal vision language models},
  author={Rajabi, Navid and Kosecka, Jana},
  journal={arXiv preprint arXiv:2308.09778},
  year={2023}
}

@article{liu2023visual,
  title={Visual spatial reasoning},
  author={Liu, Fangyu and Emerson, Guy and Collier, Nigel},
  journal={TACL},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{liu-etal-2022-things,
    title = "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
    author = "Liu, Xiao  and
      Yin, Da  and
      Feng, Yansong  and
      Zhao, Dongyan",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "ACL",
    year = "2022",
    url = "https://aclanthology.org/2022.acl-long.168",
    doi = "10.18653/v1/2022.acl-long.168",
    abstract = "Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge. Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial commonsense reasoning. Starting from the observation that images are more likely to exhibit spatial commonsense than texts, we explore whether models with visual signals learn more spatial commonsense than text-based PLMs. We propose a spatial commonsense benchmark that focuses on the relative scales of objects, and the positional relationship between people and objects under different actions. We probe PLMs and models with visual signals, including vision-language pretrained models and image synthesis models, on this benchmark, and find that image synthesis models are more capable of learning accurate and consistent spatial knowledge than other models. The spatial knowledge from image synthesis models also helps in natural language understanding tasks that require spatial commonsense.",
}

@inproceedings{mirzaee-etal-2021-spartqa,
    title = "{SPARTQA}: A Textual Question Answering Benchmark for Spatial Reasoning",
    author = "Mirzaee, Roshanak  and
      Rajaby Faghihi, Hossein  and
      Ning, Qiang  and
      Kordjamshidi, Parisa",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.364",
    doi = "10.18653/v1/2021.naacl-main.364",
    abstract = "This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs{'} capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.",
}


@inproceedings{collell2018acquiring,
  title={Acquiring common sense spatial knowledge through implicit spatial templates},
  author={Collell, Guillem and Van Gool, Luc and Moens, Marie-Francine},
  booktitle={AAAI},
  volume={32},
  number={1},
  year={2018}
}

@incollection{freksa1991qualitative,
  title={Qualitative spatial reasoning},
  author={Freksa, Christian},
  booktitle={Cognitive and linguistic aspects of geographic space},
  pages={361--372},
  year={1991}
}

@inproceedings{cho2023spatially,
  title={Spatially-Aware Transformers for Embodied Agents},
  author={Cho, Junmo and Yoon, Jaesik and Ahn, Sungjin},
  booktitle={ICLR},
  year={2023}
}

@article{rozanova2021grounding,
  title={Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?},
  author={Rozanova, Julia and Ferreira, Deborah and Dubba, Krishna and Cheng, Weiwei and Zhang, Dell and Freitas, Andre},
  journal={arXiv preprint arXiv:2109.08634},
  year={2021}
}

@article{chen2024internvl2,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@article{zhang2024longva,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={ICLR},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@inproceedings{majumdar2024openeqa,
  title={Openeqa: Embodied question answering in the era of foundation models},
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  booktitle={CVPR},
  year={2024}
}

@article{baddeley1992working,
  title={Working memory},
  author={Baddeley, Alan},
  journal={Science},
  volume={255},
  number={5044},
  pages={556--559},
  year={1992}
}

@inproceedings{parcalabescu2024measuring,
  title={On measuring faithfulness or self-consistency of natural language explanations},
  author={Parcalabescu, Letitia and Frank, Anette},
  booktitle={ACL},
  year={2024}
}


@inproceedings{lyu-etal-2023-faithful,
    title = "Faithful Chain-of-Thought Reasoning",
    author = "Lyu, Qing  and
      Havaldar, Shreya  and
      Stein, Adam  and
      Zhang, Li  and
      Rao, Delip  and
      Wong, Eric  and
      Apidianaki, Marianna  and
      Callison-Burch, Chris",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "ACL",
    year = "2023"
}


@inproceedings{gao-etal-2024-self,
    title = "Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models",
    author = "Gao, Haoyu  and
      Lin, Ting-En  and
      Li, Hangyu  and
      Yang, Min  and
      Wu, Yuchuan  and
      Ma, Wentao  and
      Huang, Fei  and
      Li, Yongbin",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "COLING",
    year = "2024",
}


@article{
oquab2024dinov,
title={{DINO}v2: Learning Robust Visual Features without Supervision},
author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
journal={TMLR},
year={2024},
}

@article{Zhou2018open3d,
   author  = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
   title   = {{Open3D}: {A} Modern Library for {3D} Data Processing},
   journal = {arXiv:1801.09847},
   year    = {2018},
}

@article{zhang2024lmms,
  title={Lmms-eval: Reality check on the evaluation of large multimodal models},
  author={Zhang, Kaichen and Li, Bo and Zhang, Peiyuan and Pu, Fanyi and Cahyono, Joshua Adrian and Hu, Kairui and Liu, Shuai and Zhang, Yuanhan and Yang, Jingkang and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2407.12772},
  year={2024}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
