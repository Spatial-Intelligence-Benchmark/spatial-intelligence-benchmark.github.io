@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@misc{lei2022revealingsingleframebias,
      title={Revealing Single Frame Bias for Video-and-Language Learning}, 
      author={Jie Lei and Tamara L. Berg and Mohit Bansal},
      year={2022},
      eprint={2206.03428},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.03428}, 
}

@misc{ramakrishnan2024doesspatialcognitionemerge,
      title={Does Spatial Cognition Emerge in Frontier Models?}, 
      author={Santhosh Kumar Ramakrishnan and Erik Wijmans and Philipp Kraehenbuehl and Vladlen Koltun},
      year={2024},
      eprint={2410.06468},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.06468}, 
}

@misc{xu2024mcbenchbenchmarkmulticontextvisual,
      title={MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs}, 
      author={Yunqiu Xu and Linchao Zhu and Yi Yang},
      year={2024},
      eprint={2410.12332},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.12332}, 
}

@inproceedings{dai2017scannet,
  title={Scannet: Richly-annotated 3d reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5828--5839},
  year={2017}
}

@inproceedings{yeshwanth2023scannet++,
  title={Scannet++: A high-fidelity dataset of 3d indoor scenes},
  author={Yeshwanth, Chandan and Liu, Yueh-Cheng and Nie{\ss}ner, Matthias and Dai, Angela},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12--22},
  year={2023}
}

@article{baruch2021arkitscenes,
  title={Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data},
  author={Baruch, Gilad and Chen, Zhuoyuan and Dehghan, Afshin and Dimry, Tal and Feigin, Yuri and Fu, Peter and Gebauer, Thomas and Joffe, Brandon and Kurz, Daniel and Schwartz, Arik and others},
  journal={arXiv preprint arXiv:2111.08897},
  year={2021}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{huang2023can,
  title={Can large language models explain themselves? a study of llm-generated self-explanations},
  author={Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2310.11207},
  year={2023}
}

@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  pages={303--338},
  year={2010},
  publisher={Springer}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@book{salton1986introduction,
author = {Salton, Gerard and McGill, Michael J.},
title = {Introduction to Modern Information Retrieval},
year = {1986},
isbn = {0070544840},
publisher = {McGraw-Hill, Inc.},
address = {USA}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@book{gardner1983frames,
  author       = {Howard Gardner},
  title        = {Frames of Mind: The Theory of Multiple Intelligences},
  year         = {1983},
  edition      = {Tenth-anniversary Edition, Second Paperback Edition},
  publisher    = {Basic Books}
}

@misc{naveed2024comprehensiveoverviewlargelanguage,
      title={A Comprehensive Overview of Large Language Models}, 
      author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
      year={2024},
      eprint={2307.06435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.06435}, 
}

@article{beguvs2023large,
  title={Large linguistic models: Analyzing theoretical linguistic abilities of LLMs},
  author={Begu{\v{s}}, Ga{\v{s}}per and D{\k{a}}bkowski, Maksymilian and Rhodes, Ryan},
  journal={arXiv preprint arXiv:2305.00948},
  year={2023}
}

@misc{zhao2023unveilingcorelinguisticregion,
      title={Unveiling A Core Linguistic Region in Large Language Models}, 
      author={Jun Zhao and Zhihao Zhang and Yide Ma and Qi Zhang and Tao Gui and Luhui Gao and Xuanjing Huang},
      year={2023},
      eprint={2310.14928},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.14928}, 
}

@misc{wei2022emergentabilitieslargelanguage,
      title={Emergent Abilities of Large Language Models}, 
      author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
      year={2022},
      eprint={2206.07682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.07682}, 
}

@misc{kassner2023languagemodelsrationality,
      title={Language Models with Rationality}, 
      author={Nora Kassner and Oyvind Tafjord and Ashish Sabharwal and Kyle Richardson and Hinrich Schuetze and Peter Clark},
      year={2023},
      eprint={2305.14250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14250}, 
}

@article{clark1991dual,
  author    = {Clark, James M. and Paivio, Allan},
  title     = {Dual Coding Theory and Education},
  journal   = {Educational Psychology Review},
  volume    = {3},
  number    = {3},
  pages     = {149--210},
  year      = {1991},
  doi       = {10.1007/BF01320076},
  publisher = {Springer}
}

@incollection{meneghetti2022individual,
  author    = {Chiara Meneghetti and Laura Miola and Tommaso Feraco and Veronica Muffato},
  title     = {Individual Differences in Navigation: An Introductory Overview},
  booktitle = {Prime Archives in Psychology: 2nd Edition},
  editor    = {Paul Raj},
  year      = {2022},
  publisher = {Vide Leaf},
  address   = {Hyderabad, India}
}


@inbook{Newcombe2024Spatial,
	author = {Newcombe, Nora S.},
	booktitle = {Open {Encyclopedia} of {Cognitive} {Science}},
	editor = {Frank, Michael C. and Majid, Asifa},
	year = {2024},
	month = {jul 24},
	note = {https://oecs.mit.edu/pub/or750iar},
	publisher = {MIT Press},
	title = {Spatial {Cognition}},
}

@misc{chabris_jerde_woolley_gerbasi_schuldt_wai_bennett_hackman_kosslyn_2023,
 title={Spatial and Object Visualization Cognitive Styles: Validation Studies in 3839 Individuals},
 url={osf.io/preprints/psyarxiv/98hw3},
 DOI={10.31234/osf.io/98hw3},
 publisher={PsyArXiv},
 author={Chabris, Christopher F and Jerde, Thomas and Woolley, Anita W and Gerbasi, Margaret E and Schuldt, Jonathon P and Wai, Jonathan and Bennett, Sean L and Hackman, J. R and Kosslyn, Stephen M},
 year={2023},
 month={Sep}
}

@article{tolman1948cognitive,
  author    = {Tolman, E. C.},
  title     = {Cognitive maps in rats and men},
  journal   = {Psychological Review},
  year      = {1948},
  volume    = {55},
  number    = {4},
  pages     = {189--208},
  doi       = {10.1037/h0061626}
}

@article{mcafoose2009exploring,
  author    = {McAfoose, Julia and Baune, Bernhard T.},
  title     = {Exploring Visual–Spatial Working Memory: A Critical Review of Concepts and Models},
  journal   = {Neuropsychology Review},
  year      = {2009},
  volume    = {19},
  number    = {2},
  pages     = {130--142},
  doi       = {10.1007/s11065-008-9063-0},
  month     = {March}
}

@book{dehn2011working,
  author    = {Dehn, Milton J.},
  title     = {Working Memory and Academic Learning: Assessment and Intervention},
  year      = {2011},
  publisher = {John Wiley \& Sons},
  pages     = {80},
  isbn      = {9781118045169}
}

@article{tong2024cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@article{hurst2024gpto,
  title={GPT-4o System Card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@inproceedings{lin2024vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26689--26699},
  year={2024}
}

@article{xue2024longvila,
  title={Longvila: Scaling long-context visual language models for long videos},
  author={Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others},
  journal={arXiv preprint arXiv:2408.10188},
  year={2024}
}

@article{li2024llavaov,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@misc{zhang2024llavanextvideo,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}

@article{jimenez2023swe,
  title={Swe-bench: Can language models resolve real-world github issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{schulman2022chatgpt,
  title={{ChatGPT: Optimizing language models for dialogue}},
  author={Schulman, John and Zoph, Barret and Kim, Christina and Hilton, Jacob and Menick, Jacob and Weng, Jiayi and Uribe, Juan Felipe Ceron and Fedus, Liam and Metz, Luke and Pokorny, Michael and others},
  journal={OpenAI blog},
  year={2022}
}

@article{tian2024drivevlm,
  title={Drivevlm: The convergence of autonomous driving and large vision-language models},
  author={Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Wang, Yang and Zhao, Zhiyong and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang},
  journal={arXiv preprint arXiv:2402.12289},
  year={2024}
}

@inproceedings{shao2024lmdrive,
  title={Lmdrive: Closed-loop end-to-end driving with large language models},
  author={Shao, Hao and Hu, Yuxuan and Wang, Letian and Song, Guanglu and Waslander, Steven L and Liu, Yu and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15120--15130},
  year={2024}
}

@article{brohan2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2307.15818},
  year={2023}
}

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@article{o2023open,
  title={Open x-embodiment: Robotic learning datasets and rt-x models},
  author={O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others},
  journal={arXiv preprint arXiv:2310.08864},
  year={2023}
}

@article{wang2023voyager,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2305.16291},
  year={2023}
}

@book{nadel2008hippocampus,
  author    = {Nadel, Lynn},
  title     = {The Hippocampus and Context Revisited},
  year      = {2008},
  publisher = {Oxford University Press},
  doi       = {10.1093/acprof:oso/9780195323245.001.0001},
  isbn      = {978-0-19-986926-8}
}

@inproceedings{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11888--11898},
  year={2023}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

@article{damonlpsg2023videollama,
  author = {Zhang, Hang and Li, Xin and Bing, Lidong},
  title = {Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  year = 2023,
  journal = {arXiv preprint arXiv:2306.02858},
  url = {https://arxiv.org/abs/2306.02858}
}
@article{2023videochat,
  title={VideoChat: Chat-Centric Video Understanding},
  author={Li, Kunchang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{Ren2023TimeChat,
  title={TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding},
  author={Shuhuai Ren and Linli Yao and Shicheng Li and Xu Sun and Lu Hou},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.02051},
}

@inproceedings{huang2024vtimellm,
  title={Vtimellm: Empower llm to grasp video moments},
  author={Huang, Bin and Wang, Xin and Chen, Hong and Song, Zihan and Zhu, Wenwu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14271--14280},
  year={2024}
}
@INPROCEEDINGS{7298698,
  author={Heilbron, Fabian Caba and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={ActivityNet: A large-scale video benchmark for human activity understanding}, 
  year={2015},
  volume={},
  number={},
  pages={961-970},
  keywords={Benchmark testing;Taxonomy;Cleaning;Semantics;Organizations;Complexity theory;YouTube},
  doi={10.1109/CVPR.2015.7298698}
}


@article{Goyal2017TheS,
  title={The “Something Something” Video Database for Learning and Evaluating Visual Common Sense},
  author={Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and Susanne Westphal and Heuna Kim and Valentin Haenel and Ingo Fr{\"u}nd and Peter N. Yianilos and Moritz Mueller-Freitag and Florian Hoppe and Christian Thurau and Ingo Bax and Roland Memisevic},
  journal={2017 IEEE International Conference on Computer Vision (ICCV)},
  year={2017},
  pages={5843-5851},
  url={https://api.semanticscholar.org/CorpusID:834612}
}


@misc{tang2024sparklemasteringbasicspatial,
      title={Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning}, 
      author={Yihong Tang and Ao Qu and Zhaokai Wang and Dingyi Zhuang and Zhaofeng Wu and Wei Ma and Shenhao Wang and Yunhan Zheng and Zhan Zhao and Jinhua Zhao},
      year={2024},
      eprint={2410.16162},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.16162}, 
}
@misc{chen2024spatialvlmendowingvisionlanguagemodels,
      title={SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities}, 
      author={Boyuan Chen and Zhuo Xu and Sean Kirmani and Brian Ichter and Danny Driess and Pete Florence and Dorsa Sadigh and Leonidas Guibas and Fei Xia},
      year={2024},
      eprint={2401.12168},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.12168}, 
}
@misc{cheng2024spatialrgptgroundedspatialreasoning,
      title={SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models}, 
      author={An-Chieh Cheng and Hongxu Yin and Yang Fu and Qiushan Guo and Ruihan Yang and Jan Kautz and Xiaolong Wang and Sifei Liu},
      year={2024},
      eprint={2406.01584},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.01584}, 
}

@article{cai2024spatialbot,
  title={SpatialBot: Precise Spatial Understanding with Vision Language Models},
  author={Cai, Wenxiao and Ponomarenko, Yaroslav and Yuan, Jianhao and Li, Xiaoqi and Yang, Wankou and Dong, Hao and Zhao, Bo},
  journal={arXiv preprint arXiv:2406.13642},
  year={2024}
}

@misc{liu2024coarsecorrespondenceelicit3d,
      title={Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal Language Model}, 
      author={Benlin Liu and Yuhao Dong and Yiqin Wang and Yongming Rao and Yansong Tang and Wei-Chiu Ma and Ranjay Krishna},
      year={2024},
      eprint={2408.00754},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.00754}, 
}

@misc{yang2023setofmarkpromptingunleashesextraordinary,
      title={Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V}, 
      author={Jianwei Yang and Hao Zhang and Feng Li and Xueyan Zou and Chunyuan Li and Jianfeng Gao},
      year={2023},
      eprint={2310.11441},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2310.11441}, 
}
@article{zhu2024llava,
  title={LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness},
  author={Zhu, Chenming and Wang, Tai and Zhang, Wenwei and Pang, Jiangmiao and Liu, Xihui},
  journal={arXiv preprint arXiv:2409.18125},
  year={2024}
}

@misc{fu2024mmecomprehensiveevaluationbenchmark,
      title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
      author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
      year={2024},
      eprint={2306.13394},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.13394}, 
}

@misc{yu2023mmvetevaluatinglargemultimodal,
      title={MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities}, 
      author={Weihao Yu and Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and Zicheng Liu and Xinchao Wang and Lijuan Wang},
      year={2023},
      eprint={2308.02490},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.02490}, 
}
@misc{liu2024mmbenchmultimodalmodelallaround,
      title={MMBench: Is Your Multi-modal Model an All-around Player?}, 
      author={Yuan Liu and Haodong Duan and Yuanhan Zhang and Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and Kai Chen and Dahua Lin},
      year={2024},
      eprint={2307.06281},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.06281}, 
}

@misc{yue2024mmmumassivemultidisciplinemultimodal,
      title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI}, 
      author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
      year={2024},
      eprint={2311.16502},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.16502}, 
}

@misc{li2024multimodalarxivdatasetimproving,
      title={Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models}, 
      author={Lei Li and Yuqi Wang and Runxin Xu and Peiyi Wang and Xiachong Feng and Lingpeng Kong and Qi Liu},
      year={2024},
      eprint={2403.00231},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.00231}, 
}

@article{fu2024video,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}
@misc{li2024mvbenchcomprehensivemultimodalvideo,
      title={MVBench: A Comprehensive Multi-modal Video Understanding Benchmark}, 
      author={Kunchang Li and Yali Wang and Yinan He and Yizhuo Li and Yi Wang and Yi Liu and Zun Wang and Jilan Xu and Guo Chen and Ping Luo and Limin Wang and Yu Qiao},
      year={2024},
      eprint={2311.17005},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17005}, 
}

@misc{ye2024mmegobuildingegocentricmultimodal,
      title={MM-Ego: Towards Building Egocentric Multimodal LLMs}, 
      author={Hanrong Ye and Haotian Zhang and Erik Daxberger and Lin Chen and Zongyu Lin and Yanghao Li and Bowen Zhang and Haoxuan You and Dan Xu and Zhe Gan and Jiasen Lu and Yinfei Yang},
      year={2024},
      eprint={2410.07177},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.07177}, 
}

@misc{liu2024tempcompassvideollmsreally,
      title={TempCompass: Do Video LLMs Really Understand Videos?}, 
      author={Yuanxin Liu and Shicheng Li and Yi Liu and Yuxiang Wang and Shuhuai Ren and Lei Li and Sishuo Chen and Xu Sun and Lu Hou},
      year={2024},
      eprint={2403.00476},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.00476}, 
}

@misc{li2024vitatecsdiagnosticdatasettemporal,
      title={VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models}, 
      author={Shicheng Li and Lei Li and Shuhuai Ren and Yuanxin Liu and Yi Liu and Rundong Gao and Xu Sun and Lu Hou},
      year={2024},
      eprint={2311.17404},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17404}, 
}

@misc{chen2024rextimebenchmarksuitereasoningacrosstime,
      title={ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos}, 
      author={Jr-Jen Chen and Yu-Chien Liao and Hsi-Che Lin and Yu-Chu Yu and Yen-Chun Chen and Yu-Chiang Frank Wang},
      year={2024},
      eprint={2406.19392},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.19392}, 
}

@misc{shangguan2024tomatoassessingvisualtemporal,
      title={TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models}, 
      author={Ziyao Shangguan and Chuhan Li and Yuxuan Ding and Yanan Zheng and Yilun Zhao and Tesca Fitzgerald and Arman Cohan},
      year={2024},
      eprint={2410.23266},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.23266}, 
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{xu2024drivegpt4,
  title={Drivegpt4: Interpretable end-to-end autonomous driving via large language model},
  author={Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K and Li, Zhenguo and Zhao, Hengshuang},
  journal={IEEE Robotics and Automation Letters},
  year={2024},
  publisher={IEEE}
}

@article{chandrasegaran2024hourvideo,
  title={HourVideo: 1-Hour Video-Language Understanding},
  author={Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M and Kota, Taran and He, Jimming and Eyzaguirre, Crist{\'o}bal and Durante, Zane and Li, Manling and Wu, Jiajun and Fei-Fei, Li},
  journal={arXiv preprint arXiv:2411.04998},
  year={2024}
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46212--46244},
  year={2023}
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18995--19012},
  year={2022}
}

@article{gur2023real,
  title={A real-world webagent with planning, long context understanding, and program synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  journal={arXiv preprint arXiv:2307.12856},
  year={2023}
}

@article{shepard1988mental,
  title={Mental rotation: effects of dimensionality of objects and type of task.},
  author={Shepard, Shenna and Metzler, Douglas},
  journal={Journal of experimental psychology: Human perception and performance},
  volume={14},
  number={1},
  pages={3},
  year={1988},
  publisher={American Psychological Association}
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{bai2023qwenvl,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@article{wang2024qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}


@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11975--11986},
  year={2023}
}

@article{mu2024embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2024world,
  title={World model on million-length video and language with ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2402.08268},
  year={2024}
}

@article{kim2024openvla,
  title={OpenVLA: An Open-Source Vision-Language-Action Model},
  author={Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others},
  journal={arXiv preprint arXiv:2406.09246},
  year={2024}
}

@inproceedings{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@inproceedings{chen2024spatialvlm,
  title={Spatialvlm: Endowing vision-language models with spatial reasoning capabilities},
  author={Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14455--14465},
  year={2024}
}

@article{cheng2024spatialrgpt,
  title={SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model},
  author={Cheng, An-Chieh and Yin, Hongxu and Fu, Yang and Guo, Qiushan and Yang, Ruihan and Kautz, Jan and Wang, Xiaolong and Liu, Sifei},
  journal={arXiv preprint arXiv:2406.01584},
  year={2024}
}

@article{liu2024coarse,
  title={Coarse correspondence elicit 3d spacetime understanding in multimodal language model},
  author={Liu, Benlin and Dong, Yuhao and Wang, Yiqin and Rao, Yongming and Tang, Yansong and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2408.00754},
  year={2024}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}

@inproceedings{liu2025mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={European Conference on Computer Vision},
  pages={216--233},
  year={2025},
  organization={Springer}
}

@inproceedings{li2024seed,
  title={SEED-Bench: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13299--13308},
  year={2024}
}

@article{yu2023mmvet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@article{fu2024videomme,
  title={Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22195--22206},
  year={2024}
}

@article{ning2023videobench,
  title={Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models},
  author={Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li},
  journal={arXiv preprint arXiv:2311.16103},
  year={2023}
}

@article{liu2024tempcompass,
  title={Tempcompass: Do video llms really understand videos?},
  author={Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2403.00476},
  year={2024}
}

@article{li2023vitatecs,
  title={Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models},
  author={Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2311.17404},
  year={2023}
}

@article{fang2024mmbenchvideo,
  title={MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2406.14515},
  year={2024}
}

@article{ye2024mmego,
  title={MM-Ego: Towards Building Egocentric Multimodal LLMs},
  author={Ye, Hanrong and Zhang, Haotian and Daxberger, Erik and Chen, Lin and Lin, Zongyu and Li, Yanghao and Zhang, Bowen and You, Haoxuan and Xu, Dan and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2410.07177},
  year={2024}
}

@article{li2024topviewrs,
  title={TopViewRS: Vision-Language Models as Top-View Spatial Reasoners},
  author={Li, Chengzu and Zhang, Caiqi and Zhou, Han and Collier, Nigel and Korhonen, Anna and Vuli{\'c}, Ivan},
  journal={arXiv preprint arXiv:2406.02537},
  year={2024}
}

@article{tang2024sparkle,
  title={Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning},
  author={Tang, Yihong and Qu, Ao and Wang, Zhaokai and Zhuang, Dingyi and Wu, Zhaofeng and Ma, Wei and Wang, Shenhao and Zheng, Yunhan and Zhao, Zhan and Zhao, Jinhua},
  journal={arXiv preprint arXiv:2410.16162},
  year={2024}
}

@article{yang2024virl,
  title={V-irl: Grounding virtual intelligence in real life},
  author={Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining},
  journal={arXiv preprint arXiv:2402.03310},
  year={2024}
}

@article{ramakrishnan2024does,
  title={Does Spatial Cognition Emerge in Frontier Models?},
  author={Ramakrishnan, Santhosh Kumar and Wijmans, Erik and Kraehenbuehl, Philipp and Koltun, Vladlen},
  journal={arXiv preprint arXiv:2410.06468},
  year={2024}
}

@article{yang2023set,
  title={Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v},
  author={Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.11441},
  year={2023}
}

@article{zhu2024llava3d,
  title={LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness},
  author={Zhu, Chenming and Wang, Tai and Zhang, Wenwei and Pang, Jiangmiao and Liu, Xihui},
  journal={arXiv preprint arXiv:2409.18125},
  year={2024}
}

@article{wu2024visualization,
  title={Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models},
  author={Wu, Wenshan and Mao, Shaoguang and Zhang, Yadong and Xia, Yan and Dong, Li and Cui, Lei and Wei, Furu},
  journal={arXiv preprint arXiv:2404.03622},
  year={2024}
}

@article{buhner2008working,
  title={Working memory, visual--spatial-intelligence and their relationship to problem-solving},
  author={B{\"u}hner, Markus and Kr{\"o}ner, Stephan and Ziegler, Matthias},
  journal={Intelligence},
  volume={36},
  number={6},
  pages={672--680},
  year={2008},
  publisher={Elsevier}
}

@article{marshall2001spatial,
  title={Spatial cognition: where we were and where we are},
  author={Marshall, John C and Fink, Gereon R},
  journal={Neuroimage},
  volume={14},
  number={1},
  pages={S2--S7},
  year={2001},
  publisher={Elsevier}
}

@book{waller2013handbook,
  title={Handbook of spatial cognition.},
  author={Waller, David Ed and Nadel, Lynn Ed},
  year={2013},
  publisher={American Psychological Association}
}

@book{mallot2024geometry,
  title={From geometry to behavior: An introduction to spatial cognition},
  author={Mallot, Hanspeter A},
  year={2024},
  publisher={MIT Press}
}

@article{vasilyeva2012development,
  title={Development of spatial cognition},
  author={Vasilyeva, Marina and Lourenco, Stella F},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={3},
  number={3},
  pages={349--362},
  year={2012},
  publisher={Wiley Online Library}
}
@book{newcombe2000making,
  title={Making space: The development of spatial representation and reasoning},
  author={Newcombe, NS},
  year={2000},
  publisher={MIT Press}
}

@article{ren2016faster,
  title={Faster R-CNN: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2016},
  publisher={IEEE}
}

@article{yamada2023evaluating,
  title={Evaluating spatial understanding of large language models},
  author={Yamada, Yutaro and Bao, Yihan and Lampinen, Andrew K and Kasai, Jungo and Yildirim, Ilker},
  journal={arXiv preprint arXiv:2310.14540},
  year={2023}
}

@article{momennejad2024evaluating,
  title={Evaluating cognitive maps and planning in large language models with CogEval},
  author={Momennejad, Ida and Hasanbeig, Hosein and Vieira Frujeri, Felipe and Sharma, Hiteshi and Jojic, Nebojsa and Palangi, Hamid and Ness, Robert and Larson, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shepard1978mental,
  title={The mental image.},
  author={Shepard, Roger N},
  journal={American psychologist},
  volume={33},
  number={2},
  pages={125},
  year={1978},
  publisher={American Psychological Association}
}

@article{rajabi2023towards,
  title={Towards grounded visual spatial reasoning in multi-modal vision language models},
  author={Rajabi, Navid and Kosecka, Jana},
  journal={arXiv preprint arXiv:2308.09778},
  year={2023}
}

@article{liu2023visual,
  title={Visual spatial reasoning},
  author={Liu, Fangyu and Emerson, Guy and Collier, Nigel},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={635--651},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{liu2022things,
  title={Things not written in text: Exploring spatial commonsense from visual signals},
  author={Liu, Xiao and Yin, Da and Feng, Yansong and Zhao, Dongyan},
  journal={arXiv preprint arXiv:2203.08075},
  year={2022}
}

@article{mirzaee2021spartqa,
  title={Spartqa:: A textual question answering benchmark for spatial reasoning},
  author={Mirzaee, Roshanak and Faghihi, Hossein Rajaby and Ning, Qiang and Kordjmashidi, Parisa},
  journal={arXiv preprint arXiv:2104.05832},
  year={2021}
}

@inproceedings{collell2018acquiring,
  title={Acquiring common sense spatial knowledge through implicit spatial templates},
  author={Collell, Guillem and Van Gool, Luc and Moens, Marie-Francine},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@incollection{freksa1991qualitative,
  title={Qualitative spatial reasoning},
  author={Freksa, Christian},
  booktitle={Cognitive and linguistic aspects of geographic space},
  pages={361--372},
  year={1991},
  publisher={Springer}
}

@inproceedings{cho2023spatially,
  title={Spatially-Aware Transformers for Embodied Agents},
  author={Cho, Junmo and Yoon, Jaesik and Ahn, Sungjin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{rozanova2021grounding,
  title={Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?},
  author={Rozanova, Julia and Ferreira, Deborah and Dubba, Krishna and Cheng, Weiwei and Zhang, Dell and Freitas, Andre},
  journal={arXiv preprint arXiv:2109.08634},
  year={2021}
}

@article{chen2024internvl2,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@article{zhang2024longva,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@inproceedings{majumdar2024openeqa,
  title={Openeqa: Embodied question answering in the era of foundation models},
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16488--16498},
  year={2024}
}

@article{baddeley1992working,
  title={Working memory},
  author={Baddeley, Alan},
  journal={Science},
  volume={255},
  number={5044},
  pages={556--559},
  year={1992}
}

@inproceedings{parcalabescu2024measuring,
  title={On measuring faithfulness or self-consistency of natural language explanations},
  author={Parcalabescu, Letitia and Frank, Anette},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6048--6089},
  year={2024}
}


@article{wu2024mindseye,
  title={Mind’s eye of llms: Visualization-of-thought elicits spatial reasoning in large language models},
  author={Wu, Wenshan and Mao, Shaoguang and Zhang, Yadong and Xia, Yan and Dong, Li and Cui, Lei and Wei, Furu},
  journal={arXiv preprint arxiv:2404.03622},
  year={2024}
}

@article{lyu2023faithful,
  title={Faithful chain-of-thought reasoning},
  author={Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:2301.13379},
  year={2023}
}

@article{gao2023self,
  title={Self-explanation prompting improves dialogue understanding in large language models},
  author={Gao, Haoyu and Lin, Ting-En and Li, Hangyu and Yang, Min and Wu, Yuchuan and Ma, Wentao and Li, Yongbin},
  journal={arXiv preprint arXiv:2309.12940},
  year={2023}
}

@article{oquab2023dinov2,
  title={Dinov2: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@article{Zhou2018open3d,
   author  = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
   title   = {{Open3D}: {A} Modern Library for {3D} Data Processing},
   journal = {arXiv:1801.09847},
   year    = {2018},
}

@article{zhang2024lmms,
  title={Lmms-eval: Reality check on the evaluation of large multimodal models},
  author={Zhang, Kaichen and Li, Bo and Zhang, Peiyuan and Pu, Fanyi and Cahyono, Joshua Adrian and Hu, Kairui and Liu, Shuai and Zhang, Yuanhan and Yang, Jingkang and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2407.12772},
  year={2024}
}
