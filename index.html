<!doctype html>
<html lang="en">
    <head>
        <title>Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces</title>
        <!-- <link rel="icon" type="image/x-icon" href="/static/img/icons/jellyfish.ico"> -->

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://VSI-Bench.github.io/" />
        <meta property="og:image" content="static/img/preview.png" />
        <meta property="og:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
        <!-- <meta property="og:description" content="Cambrian-1 is a family of multimodal LLMs with a vision-centric design. We also release CV-Bench, a new vision-centric benchmark, and Cambrian-10M, a multimodal instruction-tuning dataset." /> -->
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://VSI-Bench.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="static/img/preview.png" />
        <meta name="twitter:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
        <meta name="twitter:description" content="We introduce VSI-Bench, a novel benchmark of over 5,000 video-based visual-spatial intelligence questions, to evaluate and probe MLLMs, which revealed that their emerging spatial reasoning and local world modeling capabilities remain subhuman but promising.        " />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Thinking in Space</i></h1>
                    <h2>How Multimodal Large Language Models 
                        See, Remember and Recall Spaces</h2>
                    

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>PDF</span>
                        </a>
                        <!-- replace image -->
                        <a href="" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>                      
                        <a href="" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VSI-Bench</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/preview.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>

        
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p>
                        <a href="https://jihanyang.github.io/" class="author-link" target="_blank">Jihan Yang</a> 
                        <sup>△</sup>
                    </p>
                    <p>
                        <a href="https://github.com/vealocia" class="author-link" target="_blank">Shusheng Yang</a> 
                        <sup>△*</sup>
                    </p>
                    <p>
                        <a href="https://www.linkedin.com/in/anjaliwgupta/" class="author-link" target="_blank">Anjali Gupta</a> 
                        <sup>△*</sup>
                    </p>
                    <p>
                        <a href="https://rilynhan.github.io" class="author-link" target="_blank">Rilyn Han</a> 
                        <sup>▲*</sup>
                    </p>
                    <p>
                        <a href="https://profiles.stanford.edu/fei-fei-li" class="author-link" target="_blank">Fei-Fei Li</a> 
                        <sup>★</sup>
                    </p>
                    <p>
                        <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie</a> 
                        <sup>△</sup>
                    </p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p>
                        <sup>△</sup>
                        <a href="https://cs.nyu.edu/home/index.html" class="affiliation-link" target="_blank">New York University</a>
                    </p>
                    <p>
                        <sup>▲</sup>
                        <a href="https://www.yale.edu" class="affiliation-link" target="_blank">Yale University</a>

                    </p>
                    <p>
                        <sup>★</sup>
                        <a href="https://www.stanford.edu/" class="affiliation-link" target="_blank">Stanford University</a>
                    </p>
                </div>
                <div class="byline-column">
                    <h3>Date</h3>
                    <p>
                        Dec. 1<sup>st</sup>, 2024
                    </p>
                </div>
            </div>
        </div>

        <p style="text-align: center;">
            <span class="author-note"><sup>*</sup>Equal contribution</span>&emsp;
            <!-- <span class="author-note"><sup>†</sup>Project lead</span>&emsp; -->
            <!-- <span class="author-note"><sup>†</sup>Corresponding author</span> -->
        </p>

        
        <d-figure id="fig-teaser">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="Visual-Spatial Intelligence Teaser">
                <figcaption>
                    <strong>Figure 1:</strong> Can Multimodal LLMs “think spatially” when presented with a video recording of an environment? Can they build an accurate, implicit “cognitive map” that allows them to answer questions about a space? What are the strengths and limitations of using MLLMs to enhance spatial intelligence? We dig into these questions by setting up video data for MLLMs to watch, building a VQA benchmark to check their recall, and examining what the MLLMs actually remember and understand.
                </figcaption>
            </figure>
        </d-figure>
        
        <p class="text abstract">
            We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. 
            Our evaluation reveals that MLLMs exhibit competitive visual-spatial intelligence, if still well short of human-level. 
            To understand the MLLMs' behavior, we probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models.
            <br>
        </p>
        

        

        <hr>

        <div id='vsi-benchmark' class="vsi-benchmark">
            
            <div id="sec:vsi-overview" class="sub-section">
                <h1 class="text">VSI-Bench</h1>

                    <p class="text">
                        <p class="text">
                        <strong>Benchmark Overview:</strong> We develope VSI-Bench, a benchmark to evaluate the visual-spatial intelligence of Multimodal LLMs (MLLMs) using over 5,000 question-answer pairs derived from 288 egocentric videos sourced from the validation sets of public indoor 3D scene reconstruction datasets ScanNet, ScanNet++, and ARKitScenes. VSI-Bench includes eight tasks under three task types: configurational, measurement estimation, and spatiotemporal. See Fig. 2 for an overview of the tasks in VSI-Bench and Fig. 3 for dataset statistics. Iteratively refined for quality, VSI-Bench provides a foundation to study the connection between MLLMs and 3D reconstruction.</p>
                    <d-figure id="fig-task-demo" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/task-demo.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 2: Tasks demonstration of VSI-Bench.</strong> 
                                Note: the questions above are simplified slightly for clarity and brevity.
                            </figcaption>
                        </figure>
                    </d-figure>
                    <d-figure id="fig-bench-stats">
                        <figure>
                            <!-- <div style="display: flex; align-items: center; gap: 0px; margin-left: -250px">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center; overflow: hidden; position: relative; margin: 0;">
                                    <iframe 
                                        src="static/img/dataset_overview_donut_chart.html" 
                                        style="width: 100%; height: 450px; border: none; transform: scale(0.7); transform-origin: center; padding: 0; margin: 0;" 
                                        title="Dataset Overview Donut Chart">
                                    </iframe>
                                </div>
                     -->
                                <!-- Display the image -->
                                <!-- <div style="flex: 1; height: 500px; margin-left: -300px;">
                                    <img 
                                        data-zoomable="" 
                                        draggable="false" 
                                        src="static/img/video-length.png" 
                                        alt="Fit Visualization" 
                                        style="width: 100%; height: 100%; object-fit: contain; margin: 0px; padding: 0;">
                                </div>
                            </div> -->
                            <img data-zoomable="" draggable="false" src="static/img/benchmark-stats.png" alt="benchmark category" style="width: 90%; height: auto; display: block; margin: 0 auto;">


            
                            <figcaption style="text-align: center; margin-top: 20px;">
                                <strong>Figure 3: Benchmark Statistics.</strong> 
                                <strong>Left</strong>: The distribution of tasks across three main categories. <strong>Right</strong>: The video length statistic.
                            </figcaption>
                        </figure>
                    </d-figure>
                    
            </div>
            <div id="vsi-construct" class="sub-section">
                

                    <p class="text"><strong>VSI-Bench Construction:</strong>
                        We develop a robust pipeline to construct VSI-Bench that enables high-quality question-answer (QA) pair generation at scale. Starting with data collection and unification, we standardize diverse 3D indoor scene datasets into a unified meta-information format, incorporating object categories, bounding boxes, and video specifications to support dataset-agnostic QA generation. QA pairs are generated using automated annotations from meta-information and task-specific question templates, with route planning tasks manually annotated. To ensure quality, we implement a human-in-the-loop review process, iteratively refining question templates, annotations, and QA generation rules by addressing ambiguities and errors flagged by evaluators. </p>

                    <d-figure id="fig-bench-pipeline" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/benchmark_construction.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 4: Benchmark curation pipeline.</strong> The pipeline unifies datasets into a standardized format and semantic space for consistent processing. QA pairs are then generated through both human annotation and question templates. To ensure quality, human verification is implemented at all key stages for filtering low-quality videos, annotations, and ambiguous QA pairs.
                            </figcaption>
                        </figure>
                    </d-figure>

            </div>
        <div id="sec:vsi-eval" class="vsi-evaluation">
            <h1 class="text">Evaluation on VSI-Bench</h1>

                    <!-- <p class="text">
                        <p class="text">
                        We benchmarked 15 video-supporting MLLMs from diverse model families. For proprietary models, we consider Gemini-1.5 and GPT-4o. For open-source models, we evaluate models from InternVL2, ViLA, 203 LongViLA, LongVA, LLaVA-OneVision, and LLaVA-NeXT-Video. All evaluations were conducted in zero-shot settings with default prompts and greedy decoding for reproducibility. Tasks were evaluated using either Multiple-Choice Answer (MCA) accuracy or our proposed Mean Relative Accuracy (MRA) for Numerical Answer (NA) tasks. Baselines include random selection and frequency-based answer selection to identify performance gains due to distribution biases. Additionally, human performance was assessed on a randomly sampled subset of 400 questions (VSI-Bench tiny), with metrics compared to Gemini-1.5 Pro.
                    </p> -->
                    <script type="text/javascript" id="MathJax-script" async
                    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
                    </script>
            <p class="text"><strong>Evaluation Setup</strong>:
                We benchmarked 15 video-supporting MLLMs from diverse model families. For proprietary models, we consider Gemini-1.5 and GPT-4o. For open-source models, we evaluate models from InternVL2, ViLA, LongViLA, LongVA, LLaVA-OneVision, and LLaVA-NeXT-Video. All evaluations are conducted in zero-shot settings with default prompts and greedy decoding for reproducibility. Tasks are evaluated using either Multiple-Choice Answer (MCA) accuracy or our proposed Mean Relative Accuracy (MRA) for Numerical Answer (NA) tasks.
            </p>
        
            <div class="formula">
                $$\text{MRA} = \frac{1}{10} \sum_{\theta \in C} \mathbb{1}\left(\frac{| \hat{y} - y |}{y} < 1 - \theta\right)$$
            </div>
        
            <p class="text">
                Baselines include random selection and frequency-based answer selection to identify performance gains due to distribution biases. Additionally, human performance is assessed on a randomly sampled subset of 400 questions (VSI-Bench tiny), with metrics compared to Gemini-1.5 Pro.
            </p>
                   
            
            <p class="text"><strong>Main Results</strong>:
                Human evaluators achieve an average accuracy of 79%, outperforming the best model by 33%, with near-perfect performance (94%-100%) on configuration and spatiotemporal tasks. However, the gap narrows on measurement tasks that require precise estimation, where MLLMs demonstrate relative strength in quantitative tasks. Among proprietary models, Gemini-1.5 Pro stands out, significantly exceeding chance baselines and approaching human performance in tasks like absolute distance and room size estimation, despite being trained only on 2D digital data. Top-performing open-source models, such as LLaVA-NeXT-Video-72B and LLaVA-OneVision-72B, achieve competitive results, trailing Gemini-1.5 Pro by just 4%-5%. However, most open-source models (7/12) fall below chance baselines, revealing notable deficiencies in visual-spatial intelligence.
            </p>
            <d-figure id="fig-eval-bench" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/eval-bench.png" alt="benchmark category">
                    <figcaption>
                        <strong>Table 1:  Evaluation on VSI-Bench.</strong> <strong>Left</strong>: Dark gray indicates the best result among all models and light gray indicates the best
                        result among open-source models. † indicates results on VSI-Bench (tiny) set. <strong>Right</strong>: Results including the top-3 open-source models.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text"><strong>Blind Evaluation</strong>:
                We compare MLLMs’ performance against “Chance Level (frequency)” and “Vision Disabled” (blind) results, averaged across six top models (three open-source and three closed-source). The consistent improvements in “Enabled−Disabled” and the general degradation in “Disabled−Chance” highlight the importance of video input for VSI-Bench, as blind models perform worse than chance. However, MLLMs struggle to surpass chance level on tasks such as absolute distance estimation, route planning, and relative direction, reflecting the inherent difficulty of these tasks. Interestingly, “Vision Disabled” models significantly outperform chance on object size tasks, likely due to the integration of common-sense knowledge from language model training.
            </p>

            <d-figure id="fig-blind-eval" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/blind_evaluation.png" alt="benchmark category" style="width: 70%; display: block; margin: 0 auto;">
                    <figcaption>
                        <strong>Figure 5</strong>: Performance comparisons between Vision Enabled
                        (w/ video), Vision Disabled (w/o video) and Chance Level
                        (Freq.). 
                    </figcaption>
                </figure>
            </d-figure>

            <div id='visual-representation' class="viusal-representation-block">
            <h1 class="text">How MLLMs Think in Space Linguistically</h1>
                
                <p class="text">
                    To better understand when and why models succeed or fail and to elucidate the facets of visual-spatial intelligence they possess, we examine how MLLMs think in space linguistically.
                </p>

                              
                </d-figure>
                <p class="text">
                    <strong>Case Studies</strong>:
                    In the success example, the model demonstrates advanced video understanding with accurate timestamped descriptions and a correct step-by-step reasoning process. The use of a global coordinate system suggests that MLLMs may construct implicit world models by integrating spatial context and reasoning. In the error case, the model fails in egocentric-allocentric transformation, incorrectly interpreting a video sequence due to reliance on the egocentric view, leading to a flawed spatial inference.
                </p>

                <d-figure id="fig-case-study" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/case-study.png" alt="benchmark category" style="width: 90%; display: block; margin: 0 auto;">
                        <figcaption>
                            <strong>Figure 6</strong>: Examples of how a MLLM thinks as seen in self-explanations. 
                        </figcaption>
                    </figure>
                </d-figure>

            
                <p class="text">
                    <strong>Error Analysis</strong>:
                    Analysis of errors from the best-performing MLLM on VSI-Bench (tiny) identifies four main error types: visual perception, linguistic intelligence, relational reasoning, and egocentric-allocentric transformation. Figure 6 reveals that 71% of errors stem from spatial reasoning, particularly in understanding distance, size, and direction. This indicates that spatial reasoning remains the key bottleneck for improving MLLM performance on VSI-Bench.
                </p>
                <d-figure id="fig-error-breakdown" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/error_breakdown.png" alt="benchmark category" style="width: 50%; display: block; margin: 0 auto;">
                        <figcaption>
                            <strong>Figure 7</strong>: Human-conducted analysis of errors by type. 
                        </figcaption>
                    </figure>
                </d-figure>

                <div class="highlight-box">
                    <em>Findings 1: Spatial reasoning is the primary bottleneck for MLLM performance on VSI-Bench.</em>
                </div>
                

                <p class="text">
                    <strong>Limits of CoT Methods in Visuospatial Tasks</strong>:
                    We investigated three prompting techniques—Zero-Shot Chain-of-Thought (CoT), Self-Consistency with CoT, and Tree-of-Thoughts (ToT)—to improve MLLM reasoning on VSI-Bench. Surprisingly, all three methods led to performance degradation, with Zero-Shot CoT and ToT reducing average performance by 4%, and Self-Consistency falling 1.1% below the baseline. While tasks like appearance order and absolute distance estimation saw slight improvements due to reduced linguistic errors, tasks like room size and object size suffered significant declines (8%-21%), indicating that prompting for deeper reasoning can be unreliable and even detrimental for visual-spatial tasks.
                </p>
                <d-figure id="fig-zero-shot" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/zero-shot.png" alt="benchmark category" style="width: 90%; display: block; margin: 0 auto;">
                        <figcaption>
                            <strong>Figure 8</strong>: Relative improvements of CoT, self-consistency and
                            Tree-of-Thought compared to the baseline. 
                        </figcaption>
                    </figure>
                </d-figure>
                <div class="highlight-box">
                    <em>Findings 2: Linguistic prompting techniques, although effective in language reasoning tasks, are primarily harmful for spatial reasoning.</em>
                </div>

                
                <h1 class="text"> How MLLMs Think in Space Visually</h1>
                <p class="text">
                    Since humans subconsciously build mental representations of space when reasoning spatially, we explore how MLLMs remember spaces. </p>

                    <p class="text">
                        <strong> Probing via Cognitive Maps</strong>:
                        We evaluate MLLMs’ ability to create cognitive maps, a framework for spatial representation, by prompting Gemini-1.5 Pro to predict object center positions within a 10 × 10 grid based on video input. Accuracy was measured by comparing predicted object distances with ground truth maps, considering deviations within one grid unit as correct. The model achieved 64% accuracy in positioning adjacent objects, demonstrating strong local spatial awareness but struggling with larger distances, reflecting challenges in forming global spatial representations from discrete video frames. 
                    </p>

                    <d-figure id="fig-cog-map" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/cog-map.png" alt="benchmark category" style="width: 90%; display: block; margin: 0 auto;">
                            <figcaption>
                                <strong>Figure 9</strong>. <strong>Left</strong>: Visualizations of cognitive maps from MLLM and GT. <strong>Right</strong>: Locality of the MLLM’s predicted cognitive maps.
                            </figcaption>
                        </figure>
                    </d-figure>
                    <div class="highlight-box">
                        <em>Findings 3: When remembering spaces, a MLLM forms a series of local world models in its mind from a given video, rather than a unified global model.</em>
                    </div>

                    <p class="text">
                        <strong>Better Distance Reasoning via Cognitive Maps</strong>:
                        We explored whether cognitive maps could enhance MLLMs’ spatial reasoning by prompting Gemini-1.5 Pro to generate a map from video input and use it to answer relative distance questions. Results showed a 10% accuracy improvement with the model’s own map and a 20%-32% gain using ground truth maps, highlighting the value of accurate mental imagery for enforcing global scene topology. This suggests cognitive mapping as a promising approach to improve MLLMs’ visual-spatial reasoning. 
                    </p>
                    <div style="display: flex; justify-content: center; align-items: flex-start; gap: 20px;">
                        <!-- Table (a) -->
                        <table border="1" style="border-collapse: collapse; text-align: center;">
                            <caption style="caption-side: bottom; font-style: italic;">(a) Cognitive map prompting.</caption>
                            <thead>
                                <tr>
                                    <th>Case</th>
                                    <th>Rel. Dist Acc.</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>w/o Cog. map</td>
                                    <td>46.0</td>
                                </tr>
                                <tr>
                                    <td>w/ Cog. map</td>
                                    <td>56.0</td>
                                </tr>
                                <tr>
                                    <td>w/ Cog. map (GT)</td>
                                    <td>66.0</td>
                                </tr>
                            </tbody>
                        </table>
                    
                        <!-- Table (b) -->
                        <table border="1" style="border-collapse: collapse; text-align: center;">
                            <caption style="caption-side: bottom; font-style: italic;">(b) Cognitive map canvas size.</caption>
                            <thead>
                                <tr>
                                    <th>Cog. Map Src.</th>
                                    <th>Size</th>
                                    <th>Rel. Dist Acc.</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>MLLM</td>
                                    <td>10 × 10</td>
                                    <td>56.0</td>
                                </tr>
                                <tr>
                                    <td>MLLM</td>
                                    <td>20 × 20</td>
                                    <td>54.0</td>
                                </tr>
                                <tr>
                                    <td>GT</td>
                                    <td>10 × 10</td>
                                    <td>66.0</td>
                                </tr>
                                <tr>
                                    <td>GT</td>
                                    <td>20 × 20</td>
                                    <td>78.0</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    
                    <!-- Combined Table Caption -->
                    <p style="text-align: center; font-weight: bold;">Table 2. Relative distance task with cognitive map.</p>
                    

        </div>
        </div>

       

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                We study how models see, remember, and recall spaces by building VSI-Bench and investigating the performance 
                and behavior of MLLMs on it. Our analysis of how MLLMs think in space linguistically and visually identifies existing strengths (e.g., prominent perceptual, temporal, and linguistic abilities) and bottlenecks for visual-spatial intelligence (e.g., egocentric-allocentric transformation and relational reasoning). While prevailing linguistic prompting methods fail to improve spatial reasoning, building explicit cognitive maps does enhance the spatial distance reasoning of MLLMs.

            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{yang2024think,<br>
                    &nbsp;&nbsp;title={{Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces}},<br>
                    &nbsp;&nbsp;author={Yang, Jihan and Yang, Shusheng and Gupta, Anjali and Han, Rilyn and Fei-Fei, Li and Xie, Saining},<br>
                    &nbsp;&nbsp;year={2024},<br>
                    &nbsp;&nbsp;journal={arXiv preprint},<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>   
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
    </body>
</html>
