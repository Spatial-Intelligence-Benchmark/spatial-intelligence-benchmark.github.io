<!doctype html>
<html lang="en">
    <head>
        <title>Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces</title>
        <!-- <link rel="icon" type="image/x-icon" href="/static/img/icons/jellyfish.ico"> -->

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://VSI-Bench.github.io/" />
        <meta property="og:image" content="static/img/preview.png" />
        <meta property="og:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
        <!-- <meta property="og:description" content="Cambrian-1 is a family of multimodal LLMs with a vision-centric design. We also release CV-Bench, a new vision-centric benchmark, and Cambrian-10M, a multimodal instruction-tuning dataset." /> -->
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://VSI-Bench.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="static/img/preview.png" />
        <meta name="twitter:title" content="Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces" />
        <meta name="twitter:description" content="We introduce VSI-Bench, a novel benchmark of over 5,000 video-based visual-spatial intelligence questions, to evaluate and probe MLLMs, which revealed that their emerging spatial reasoning and local world modeling capabilities remain subhuman but promising.        " />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Thinking in Space</i></h1>
                    <h2>How Multimodal Large Language Models 
                        See, Remember and Recall Spaces</h2>
                    

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>PDF</span>
                        </a>
                        <!-- replace image -->
                        <a href="" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>                      
                        <a href="" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VSI-Bench</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/preview.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>

        
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p>
                        <a href="https://jihanyang.github.io/" class="author-link" target="_blank">Jihan Yang</a> 
                        <sup>△†</sup>
                    </p>
                    <p>
                        <a href="https://github.com/vealocia" class="author-link" target="_blank">Shusheng Yang</a> 
                        <sup>△*</sup>
                    </p>
                    <p>
                        <a href="https://www.linkedin.com/in/anjaliwgupta/" class="author-link" target="_blank">Anjali Gupta</a> 
                        <sup>△*</sup>
                    </p>
                    <p>
                        <a href="https://rilynhan.github.io" class="author-link" target="_blank">Rilyn Han</a> 
                        <sup>▲*</sup>
                    </p>
                    <p>
                        <a href="https://profiles.stanford.edu/fei-fei-li" class="author-link" target="_blank">Fei-Fei Li</a> 
                        <sup>★</sup>
                    </p>
                    <p>
                        <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie</a> 
                        <sup>△</sup>
                    </p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p>
                        <sup>△</sup>
                        <a href="https://cs.nyu.edu/home/index.html" class="affiliation-link" target="_blank">New York University</a>
                    </p>
                    <p>
                        <sup>▲</sup>
                        <a href="https://www.yale.edu" class="affiliation-link" target="_blank">Yale University</a>

                    </p>
                    <p>
                        <sup>★</sup>
                        <a href="https://www.stanford.edu/" class="affiliation-link" target="_blank">Stanford University</a>
                    </p>
                </div>
                <div class="byline-column">
                    <h3>Date</h3>
                    <p>
                        Dec. 1<sup>st</sup>, 2024
                    </p>
                </div>
            </div>
        </div>

        <p style="text-align: center;">
            <span class="author-note"><sup>*</sup>Equal contribution</span>&emsp;
            <span class="author-note"><sup>†</sup>Project lead</span>&emsp;
            <!-- <span class="author-note"><sup>†</sup>Corresponding author</span> -->
        </p>

        
        <d-figure id="fig-teaser">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="Visual-Spatial Intelligence Teaser">
                <figcaption>
                    <strong>Figure 1:</strong> Can Multimodal LLMs “think spatially” when presented with a video recording of an environment? Can they build an accurate, implicit “cognitive map” that allows them to answer questions about a space? What are the strengths and limitations of using MLLMs to enhance spatial intelligence? We dig into these questions by setting up video data for MLLMs to watch, building a VQA benchmark to check their recall, and examining what the MLLMs actually remember and understand.
                </figcaption>
            </figure>
        </d-figure>
        
        <p class="text abstract">
            We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. 
            Our evaluation reveals that MLLMs exhibit competitive visual-spatial intelligence, if still well short of human-level. 
            To understand the MLLMs' behavior, we probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models.
            <br>
        </p>
        

        

        <hr>

        <div id='vsi-benchmark' class="vsi-benchmark">
            
            <div id="sec:vsi-overview" class="sub-section">
                <h1 class="text">VSI-Bench</h1>

                    <p class="text">
                        <p class="text">
                        <strong>Benchmark Overview:</strong> We develope VSI-Bench, a benchmark to evaluate the visual-spatial intelligence of Multimodal LLMs (MLLMs) using over 5,000 question-answer pairs derived from 288 egocentric videos sourced from the validation sets of public indoor 3D scene reconstruction datasets ScanNet, ScanNet++, and ARKitScenes. VSI-Bench includes eight tasks under three task types: configurational, measurement estimation, and spatiotemporal. See Fig. 2 for an overview of the tasks in VSI-Bench and Fig. 3 for dataset statistics. Iteratively refined for quality, VSI-Bench provides a foundation to study the connection between MLLMs and 3D reconstruction.</p>
                    <d-figure id="fig-task-demo" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/task-demo.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 2: Tasks demonstration of VSI-Bench.</strong> 
                                Note: the questions above are simplified slightly for clarity and brevity.
                            </figcaption>
                        </figure>
                    </d-figure>
                    <d-figure id="fig-bench-stats">
                        <figure>
                            <!-- <div style="display: flex; align-items: center; gap: 0px; margin-left: -250px">
                                <div style="flex: 3; display: flex; justify-content: center; align-items: center; overflow: hidden; position: relative; margin: 0;">
                                    <iframe 
                                        src="static/img/dataset_overview_donut_chart.html" 
                                        style="width: 100%; height: 450px; border: none; transform: scale(0.7); transform-origin: center; padding: 0; margin: 0;" 
                                        title="Dataset Overview Donut Chart">
                                    </iframe>
                                </div>
                     -->
                                <!-- Display the image -->
                                <!-- <div style="flex: 1; height: 500px; margin-left: -300px;">
                                    <img 
                                        data-zoomable="" 
                                        draggable="false" 
                                        src="static/img/video-length.png" 
                                        alt="Fit Visualization" 
                                        style="width: 100%; height: 100%; object-fit: contain; margin: 0px; padding: 0;">
                                </div>
                            </div> -->
                            <img data-zoomable="" draggable="false" src="static/img/benchmark-stats.png" alt="benchmark category" style="width: 90%; height: auto; display: block; margin: 0 auto;">


            
                            <figcaption style="text-align: center; margin-top: 20px;">
                                <strong>Figure 3: Benchmark Statistics.</strong> 
                                <strong>Top</strong>: The distribution of tasks across three main categories. <strong>Bottom</strong>: The video length statistic.
                            </figcaption>
                        </figure>
                    </d-figure>
                    
            </div>
            <div id="vsi-construct" class="sub-section">
                

                    <p class="text"><strong>VSI-Bench Construction:</strong>
                        We develop a robust pipeline to construct VSI-Bench that enables high-quality question-answer (QA) pair generation at scale. Starting with data collection and unification, we standardize diverse 3D indoor scene datasets into a unified meta-information format, incorporating object categories, bounding boxes, and video specifications to support dataset-agnostic QA generation. QA pairs are generated using automated annotations from meta-information and task-specific question templates, with route planning tasks manually annotated. To ensure quality, we implement a human-in-the-loop review process, iteratively refining question templates, annotations, and QA generation rules by addressing ambiguities and errors flagged by evaluators. </p>

                    <d-figure id="fig-bench-pipeline" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/benchmark_construction.png" alt="benchmark category">
                            <figcaption>
                                <strong>Figure 3: Benchmark curation pipeline.</strong> The pipeline unifies datasets into a standardized format and semantic space for consistent processing. QA pairs are then generated through both human annotation and question templates. To ensure quality, human verification is implemented at all key stages for filtering low-quality videos, annotations, and ambiguous QA pairs.
                            </figcaption>
                        </figure>
                    </d-figure>

            </div>
        <div id="sec:vsi-eval" class="vsi-evaluation">
            <h1 class="text">Evaluation on VSI-Bench</h1>

                    <!-- <p class="text">
                        <p class="text">
                        We benchmarked 15 video-supporting MLLMs from diverse model families. For proprietary models, we consider Gemini-1.5 and GPT-4o. For open-source models, we evaluate models from InternVL2, ViLA, 203 LongViLA, LongVA, LLaVA-OneVision, and LLaVA-NeXT-Video. All evaluations were conducted in zero-shot settings with default prompts and greedy decoding for reproducibility. Tasks were evaluated using either Multiple-Choice Answer (MCA) accuracy or our proposed Mean Relative Accuracy (MRA) for Numerical Answer (NA) tasks. Baselines include random selection and frequency-based answer selection to identify performance gains due to distribution biases. Additionally, human performance was assessed on a randomly sampled subset of 400 questions (VSI-Bench tiny), with metrics compared to Gemini-1.5 Pro.
                    </p> -->
                    <script type="text/javascript" id="MathJax-script" async
                    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
                    </script>
            <p class="text"><strong>Evaluation Setup</strong>:
                We benchmarked 15 video-supporting MLLMs from diverse model families. For proprietary models, we consider Gemini-1.5 and GPT-4o. For open-source models, we evaluate models from InternVL2, ViLA, LongViLA, LongVA, LLaVA-OneVision, and LLaVA-NeXT-Video. All evaluations are conducted in zero-shot settings with default prompts and greedy decoding for reproducibility. Tasks are evaluated using either Multiple-Choice Answer (MCA) accuracy or our proposed Mean Relative Accuracy (MRA) for Numerical Answer (NA) tasks.
            </p>
        
            <div class="formula">
                $$\text{MRA} = \frac{1}{10} \sum_{\theta \in C} \mathbb{1}\left(\frac{| \hat{y} - y |}{y} < 1 - \theta\right)$$
            </div>
        
            <p class="text">
                Baselines include random selection and frequency-based answer selection to identify performance gains due to distribution biases. Additionally, human performance is assessed on a randomly sampled subset of 400 questions (VSI-Bench tiny), with metrics compared to Gemini-1.5 Pro.
            </p>
                   
            
            <p class="text"><strong>Main Results</strong>:
                Human evaluators achieve an average accuracy of 79%, outperforming the best model by 33%, with near-perfect performance (94%-100%) on configuration and spatiotemporal tasks. However, the gap narrows on measurement tasks that require precise estimation, where MLLMs demonstrate relative strength in quantitative tasks. Among proprietary models, Gemini-1.5 Pro stands out, significantly exceeding chance baselines and approaching human performance in tasks like absolute distance and room size estimation, despite being trained only on 2D digital data. Top-performing open-source models, such as LLaVA-NeXT-Video-72B and LLaVA-OneVision-72B, achieve competitive results, trailing Gemini-1.5 Pro by just 4%-5%. However, most open-source models (7/12) fall below chance baselines, revealing notable deficiencies in visual-spatial intelligence.
            </p>
            <d-figure id="fig-eval-bench" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/eval-bench.png" alt="benchmark category">
                    <figcaption>
                        <strong>Table 1:  Evaluation on VSI-Bench.</strong> Left: Dark gray indicates the best result among all models and light gray indicates the best
                        result among open-source models. † indicates results on VSI-Bench (tiny) set. Right: Results including the top-3 open-source models.Abs. Dist.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text"><strong>Blind Evaluation</strong>:
                We compare MLLMs’ performance against “Chance Level (frequency)” and “Vision Disabled” (blind) results, averaged across six top models (three open-source and three closed-source). The consistent improvements in “Enabled−Disabled” and the general degradation in “Disabled−Chance” highlight the importance of video input for VSI-Bench, as blind models perform worse than chance. However, MLLMs struggle to surpass chance level on tasks such as absolute distance estimation, route planning, and relative direction, reflecting the inherent difficulty of these tasks. Interestingly, “Vision Disabled” models significantly outperform chance on object size tasks, likely due to the integration of common-sense knowledge from language model training.
            </p>

            <d-figure id="fig-blind-eval" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/blind_evaluation.png" alt="benchmark category" style="width: 80%; display: block; margin: 0 auto;">
                    <figcaption>
                        <strong>Figure 4</strong>: Performance comparisons between Vision Enabled
                        (w/ video), Vision Disabled (w/o video) and Chance Level
                        (Freq.). 
                    </figcaption>
                </figure>
            </d-figure>

    </body>
</html>
